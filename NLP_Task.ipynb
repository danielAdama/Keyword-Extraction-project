{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import relevant libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import nltk\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read the Train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "#list to attach the respective data\n",
    "train_doc = []\n",
    "train_tags = []\n",
    "#directory to the text files in the train documents folder\n",
    "filenames_doc = glob('Train_docs/case*.txt')\n",
    "#directory to the text files in the train tags folder\n",
    "filenames_tag = glob('Train_tags/case*.txt')\n",
    "\n",
    "#iterate over each text file\n",
    "for file_doc in filenames_doc:\n",
    "    #open the current text file\n",
    "    with open((file_doc), 'r') as f:\n",
    "        #read that text file\n",
    "        read_doc = f.read()\n",
    "        #attach to the train_doc list\n",
    "        train_doc.append(read_doc)\n",
    "for file_tag in filenames_tag:\n",
    "    with open((file_tag), 'r') as n:\n",
    "        read_tag = n.read()\n",
    "        #attach to the train_tags list\n",
    "        train_tags.append(read_tag)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "STOP_WORDS = set(stopwords.words('english'))\n",
    "#keep data in a data frame\n",
    "train_doc = pd.DataFrame(train_doc[:])\n",
    "\n",
    "def preprocess(text):\n",
    "    \n",
    "    #lowercase\n",
    "    text=text.lower()\n",
    "    \n",
    "    #remove tags\n",
    "    text=re.sub(\"</?.*?>\",\" <> \",text)\n",
    "    \n",
    "    #remove special characters and digits\n",
    "    text=re.sub(\"(\\\\d|\\\\W)+\",\" \",text)\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert back to list\n",
    "doc = train_doc[0].tolist()\n",
    "#create a vocabulary of words, ignore words that appear\n",
    "#in 85% of documents, and eliminate stopwords\n",
    "count_vect = CountVectorizer(max_df=0.85,stop_words=STOP_WORDS, preprocessor=preprocess)\n",
    "word_count_vect = count_vect.fit_transform(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(80, 11141)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#here we have 80 rows and a vocabulary size of 12737\n",
    "word_count_vect.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['kurian',\n",
       " 'joseph',\n",
       " 'leave',\n",
       " 'granted',\n",
       " 'special',\n",
       " 'petition',\n",
       " 'civil',\n",
       " 'around',\n",
       " 'acres',\n",
       " 'land']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#check 10 words from our vocabulary\n",
    "list(count_vect.vocabulary_.keys())[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TfidfTransformer to Compute Inverse Document Frequency(IDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([4.29583687, 4.70130197, 4.70130197, ..., 4.70130197, 4.70130197,\n",
       "       4.29583687])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_transformer = TfidfTransformer(smooth_idf=True, use_idf=True, norm='max',sublinear_tf=False)\n",
    "tfidf_transformer.fit(word_count_vect)\n",
    "#here are some Inverse document frequencies(IDF)s\n",
    "tfidf_transformer.idf_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we have our IDF computed, we are now ready to compute TF-IDF and extract the top tags from the test document"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read the Test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read the test data as we did earlier\n",
    "test_doc = []\n",
    "#directory to the text files in the train documents folder\n",
    "filenames_doc = glob('Test_docs/case*.txt')\n",
    "\n",
    "#iterate over each text file\n",
    "for file_doc in filenames_doc:\n",
    "    #open the current text file\n",
    "    with open((file_doc), 'r') as f:\n",
    "        #read that text file\n",
    "        read_doc = f.read()\n",
    "        #attach to the train_doc list\n",
    "        test_doc.append(read_doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sort_coo(coo_matrix):\n",
    "    #join both the coo_matrix.col and data together\n",
    "    tuples = zip(coo_matrix.col, coo_matrix.data)\n",
    "    return sorted(tuples, key=lambda x: (x[1],x[0]), reverse=True)\n",
    " \n",
    "def extract_topn_from_vector(feature_names, sorted_items, topn=10):\n",
    "    \"\"\"get the feature names and tf-idf score top n items\"\"\"\n",
    "    \n",
    "    #use only topn items from vector\n",
    "    sorted_items = sorted_items[:topn]\n",
    "    \n",
    "    score_vals = []\n",
    "    feature_vals = []\n",
    "    \n",
    "    for idx, score in sorted_items:\n",
    "        feature_name = feature_names[idx]\n",
    "        \n",
    "        #keep track of feature names and its corresponding score \n",
    "        score_vals.append(round(score, 3))\n",
    "        feature_vals.append(feature_names[idx])\n",
    "        \n",
    "    #create a tuples of feature,score\n",
    "    results = {}\n",
    "    for idx in range(len(feature_vals)):\n",
    "        results[feature_vals[idx]]=score_vals[idx]\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# you only need to do this once\n",
    "feature_names=count_vect.get_feature_names()\n",
    "\n",
    "def get_tags(idx):\n",
    "    \n",
    "    # get the document that we want to extract tags from\n",
    "    #doc=test_doc[0]\n",
    "    \n",
    "    #generate tf-idf for the given document\n",
    "    tf_idf_vector=tfidf_transformer.transform(count_vect.transform([test_doc[idx]]))\n",
    "    \n",
    "    #sort the tf-idf vectors by descending order of scores\n",
    "    sorted_items=sort_coo(tf_idf_vector.tocoo())\n",
    "    \n",
    "    #extract only the top n; n here is 10\n",
    "    tags=extract_topn_from_vector(feature_names,sorted_items,10)\n",
    "    \n",
    "    return tags\n",
    "\n",
    "# now print the results\n",
    "def print_result(idx,tags):\n",
    "    print('===='*10+'Body'+'===='*10)\n",
    "    print(test_doc[0])\n",
    "    print('===='*10+'tags'+'===='*10)\n",
    "    for t in tags:\n",
    "        print(t,tags[t])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate tags from the test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========================================Body========================================\n",
      "\n",
      "\n",
      "P. Venkatarama Reddi, J.\n",
      "\n",
      "1. The opinion recorded by the Kerala High Court in ITR No. 16 of 1997 has given rise to this appeal filed by the Chief Commissioner of Income-tax. The dispute relates to the assessment year 1985-86. At the instance of the Revenue the following question was referred under Section 256(1) of the Income tax Act for the opinion of the High Court:\n",
      "\n",
      "\"Whether on the facts and in the circumstances of the case, the Tribunal is right in law and fact in holding that Rs. 3,02,758/- cannot be brought to tax and in doleting the addition of Rs. 3,02,758/- sustained by CIT (Appeals)?\"\n",
      "\n",
      "The High Court accepted the view of the Tribunal which partly allowed the appeal of the assessee and answered the question in favour of the assessee.\n",
      "\n",
      "2. The facts in brief are: The respondent-assessee is engaged in the business of tea, spices etc. During the assessment year 1985-86 (previous year ending on 31.3.1985) the assessee 'wrote-back' in its accounts a sum of Rs. 14,65,997/- representing the provision made during earlier years (1978-1981) towards its purchase tax liability. It appears that the liability to pay purchase tax on certain goods was in dispute and, therefore, the provision was made. Further, it appears that the assessee, in support of its claim for purchase tax relief, inter alia, relied on the decision of the Kerala High Court in Neroth Oil Mills' case. The SLP filed by the Kerala State against the decision of the High Court in the said case was rejected by this Court in November, 1984. Apparently, for that reason, the assessee thought it fit to reverse the provision made earlier towards purchase tax and therefore made the entries in the books of account during the year ending on 31.3.1985. The assessing officer added the sum of Rs. 14,65,997/- which represents the provision made towards purchase tax during the assessment years 1978-79, 1979-80 and 1980-81, treating the same as the income of the previous year ending on 31.3.1985. In the first appeal, the CIT (Appeals) held that there was no justification to include the sums which were already included in the course of reassessments made for the years 1979-80 and 1980-81. However, he upheld the addition of Rs. 3,02,758 pertaining to the assessment year 1978-79. The Appellate Commissioner held that the liability of the assessee finally ceased during the year 1985-86 in view of the rejection of SLP in Neroth Oil Mills' case in November 1984. Certain observations were also made as regards the inflexibility of the sums pertaining to assessment years 1980-81 and 1981-82 in respect of which reassessments were made. However, in this appeal, we need not go into the details thereof.\n",
      "\n",
      "3. On further appeal by the assessee, the Tribunal set aside the addition of Rs. 3,02,758/- which was upheld by the Appellate Commissioner. The Tribunal did not agree with the view taken by the first Appellate Authority that there was no cessation of liability within the meaning of Section 41(1) of the Income-tax Act during the relevant year on account of dismissal of SLP in another case. The Tribunal observed that for claiming exemption from purchase tax on the ground that transaction was in the course of export, two conditions were required to be fulfilled: (1) things purchased and exported are one and the same and (2) the purchases were against firm orders for export. Neroth Oil Mills'case was concerned only with the first aspect and not the second aspect. Therefore, the Tribunal observed that the judgment in Neroth Oil Mills' case, even if it had attained finality does not put an end to the disputed issue involved in the respondent-assessee's case. The Tribunal further noticed that as late as 1993, the sales tax department was pursuing the issue relating to purchase tax liability of the assessee from the assessment year 1974-75 onwards and the cases were still pending decision before the Sales Tax Authorities. The Tribunal pointed out that he unilateral action on the part of the assessee in writing-back the amounts could not have the effect of extinguishing the statutory liability. On reference, the High Court approved the view taken by the Tribunal and held that Section 41(1) cannot be invoked in the instant case. Hence, this appeal by revenue by Special leave.\n",
      "\n",
      "4. It may be noted that the provision was made in the books of account towards purchase tax which was under dispute and the benefit of deduction from business income was availed of in the past years in relation thereto. The same was sought to be reversed by the assessee during the year ending on 31.3.1985 for whatever reason it be. The question is whether the circumstances contemplated by Section 41(1) exists so as to enable the Revenue to take back what has been allowed earlier as business expenditure and to include such amount in the income of the relevant assessment year i.e. 1985-86. In order to apply Section 41(1) in the context of the facts obtaining in the present case, the following points are to be kept in view : (1) In the course of assessment for an earlier year, allowance or deduction has been made in respect of trading liability incurred by the assessee; (2) Subsequently, a benefit is obtained in respect of such trading liability by way of remission or cessation thereof during the year in which such event occurred; (3) in that situation the value of benefit accruing to the assessee is deemed to be the profit and gains of business which otherwise would not be his income; and (4) such value of benefit is made chargeable to income tax as the income of the previous year wherein such benefit was obtained. The High Court, agreeing with the Tribunal, rightly held that the resort to Section 41(1) could arise only if the liability of the assessee can be said to have ceased finally without the possibility of reviving it. On the facts found by the Tribunal, the Tribunal as well as the High Court were well justified in coming to the conclusion that the purchase tax liability of the assessee had not ceased finally during the year in question. Despite the finality attained by the judgment in Neroth Oil Mills'case, the other issues having bearing on the exigibility of purchase tax still remained and the dispute between the assessee and the sales-tax department was still going on. There is no material on record to rebut these factual observations made by the Tribunal. Nor can it be said that the reasons given by the Tribunal are irrelevant.\n",
      "\n",
      "5. The learned senior counsel appearing for the Income Tax Department has contended that the assessee itself took steps to write-off the liability on account of purchase tax by making necessary adjustments in the books, which itself is indicative of the fact that the liability ceased for all practical purposes and therefore, the addition of amount of Rs. 3,20,758/- deeming the same as income of the year 1985-86 under Section 41(1) is well justified of the Act. But, what the assessee has done is not conclusive. As observed by the Tribunal, an unilateral action on the part of the assessee by way of writing-off the liability in its accounts does not necessarily mean that the liability ceased in the eye of law. In fact, this is the view taken by this Court in CIT v. Suguli Sugar Works(P) Ltd. MANU/SC/0077/1999MANU/SC/0077/1999 : [1999]236ITR518(SC) . We, therefore, find no substance in the contention advanced on behalf of the appellant. Incidentally, we may mention that the controversy relates to the period anterior to the introduction of Explanation 1 to Section 41(1).\n",
      "\n",
      "6. The decision of this Court in Commissioner of Income Tax v. T.V. Sundaram Iyengar and Sons Ltd. MANU/SC/1251/1996MANU/SC/1251/1996 : [1996]222ITR344(SC) has been cited by the learned counsel for the appellant. We find no relevance of this decision to the determination of the question involved in the present case. The factual matrix and the provision of law considered therein is entirely different.\n",
      "\n",
      "7. For the reasons aforesaid, we affirm the opinion expressed by the High Court and dismiss the appeal filed by the Revenue. There shall be no order as to costs.\n",
      "\n",
      "\n",
      "========================================tags========================================\n",
      "excise 1.0\n",
      "notification 0.519\n",
      "cestat 0.454\n",
      "tribunal 0.454\n",
      "serial 0.415\n",
      "product 0.387\n",
      "customs 0.309\n",
      "length 0.29\n",
      "assessed 0.29\n",
      "revenue 0.273\n"
     ]
    }
   ],
   "source": [
    "#enter the document number, idx max is 100\n",
    "idx = 2\n",
    "tags = get_tags(idx)\n",
    "print_result(idx,tags)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate tags from the whole data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Test Document</th>\n",
       "      <th>Test Tags</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>\\n\\nP. Venkatarama Reddi, J.\\n\\n1. The opinion...</td>\n",
       "      <td>{'assessee': 1.0, 'tax': 0.745, 'income': 0.50...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>\\n\\nG.B. Pattanaik, J.\\n\\n1. This batch of Spe...</td>\n",
       "      <td>{'shops': 1.0, 'excise': 0.924, 'liquor': 0.70...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>\\n\\n1. After hearing the Learned Counsel for b...</td>\n",
       "      <td>{'excise': 1.0, 'notification': 0.519, 'cestat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\\n\\n1. This appeal is directed against the jud...</td>\n",
       "      <td>{'pw': 1.0, 'appellant': 0.807, 'trial': 0.697...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>\\n\\n1. An issue raised by the appellants was t...</td>\n",
       "      <td>{'search': 1.0, 'shop': 0.73, 'count': 0.311, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>\\n\\nVikramajit Sen, J.\\n\\n1. This Appeal assai...</td>\n",
       "      <td>{'shares': 1.0, 'buy': 0.473, 'target': 0.37, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>\\n\\nAnil R. Dave, J.\\n\\n1. Leave granted.\\n\\n2...</td>\n",
       "      <td>{'lakhs': 1.0, 'sum': 0.985, 'deposited': 0.93...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>\\n\\nAnil R. Dave, J.\\n\\n1 This appeal has been...</td>\n",
       "      <td>{'document': 1.0, 'property': 0.85, 'late': 0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>\\n\\n1. By this interlocutory application, Mr. ...</td>\n",
       "      <td>{'kerala': 1.0, 'mr': 0.674, 'dogs': 0.591, 's...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>\\n\\n1. Having heard Learned Counsel appearing ...</td>\n",
       "      <td>{'missing': 1.0, 'child': 0.878, 'director': 0...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>101 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         Test Document  \\\n",
       "0    \\n\\nP. Venkatarama Reddi, J.\\n\\n1. The opinion...   \n",
       "1    \\n\\nG.B. Pattanaik, J.\\n\\n1. This batch of Spe...   \n",
       "2    \\n\\n1. After hearing the Learned Counsel for b...   \n",
       "3    \\n\\n1. This appeal is directed against the jud...   \n",
       "4    \\n\\n1. An issue raised by the appellants was t...   \n",
       "..                                                 ...   \n",
       "96   \\n\\nVikramajit Sen, J.\\n\\n1. This Appeal assai...   \n",
       "97   \\n\\nAnil R. Dave, J.\\n\\n1. Leave granted.\\n\\n2...   \n",
       "98   \\n\\nAnil R. Dave, J.\\n\\n1 This appeal has been...   \n",
       "99   \\n\\n1. By this interlocutory application, Mr. ...   \n",
       "100  \\n\\n1. Having heard Learned Counsel appearing ...   \n",
       "\n",
       "                                             Test Tags  \n",
       "0    {'assessee': 1.0, 'tax': 0.745, 'income': 0.50...  \n",
       "1    {'shops': 1.0, 'excise': 0.924, 'liquor': 0.70...  \n",
       "2    {'excise': 1.0, 'notification': 0.519, 'cestat...  \n",
       "3    {'pw': 1.0, 'appellant': 0.807, 'trial': 0.697...  \n",
       "4    {'search': 1.0, 'shop': 0.73, 'count': 0.311, ...  \n",
       "..                                                 ...  \n",
       "96   {'shares': 1.0, 'buy': 0.473, 'target': 0.37, ...  \n",
       "97   {'lakhs': 1.0, 'sum': 0.985, 'deposited': 0.93...  \n",
       "98   {'document': 1.0, 'property': 0.85, 'late': 0....  \n",
       "99   {'kerala': 1.0, 'mr': 0.674, 'dogs': 0.591, 's...  \n",
       "100  {'missing': 1.0, 'child': 0.878, 'director': 0...  \n",
       "\n",
       "[101 rows x 2 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#generate tf-idf for all documents in your list \n",
    "tf_idf_vector = tfidf_transformer.transform(count_vect.transform(test_doc))\n",
    "\n",
    "results = []\n",
    "for i in range(tf_idf_vector.shape[0]):\n",
    "    \n",
    "    #get vector for each document\n",
    "    curr_vector=tf_idf_vector[i]\n",
    "    \n",
    "    #sort the tf-idf vector by descending order of scores\n",
    "    sorted_items=sort_coo(curr_vector.tocoo())\n",
    "\n",
    "    #extract only the top n, n=10\n",
    "    tags=extract_topn_from_vector(feature_names,sorted_items,10)\n",
    "    \n",
    "    results.append(tags)\n",
    "\n",
    "Test_doc_tags=pd.DataFrame(zip(test_doc,results),columns=['Test Document','Test Tags'])\n",
    "#save document as csv\n",
    "Test_doc_tags.to_csv('Test tags.csv')\n",
    "Test_doc_tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
